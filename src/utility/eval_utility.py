"""
Evaluation Utilities Module

This module provides functions to evaluate language models on various benchmarks:
1. MMLU - Multiple-choice language understanding
2. BBH - Big Bench Hard tasks
3. TriviaQA - Question answering
4. TruthfulQA - Measuring truthfulness
5. Fluency - Text generation quality
"""

import json
import os
import numpy as np
import torch
import re
import string
from collections import Counter
import scipy
import nltk
from nltk.tokenize import word_tokenize
import logging
from src.models.model_loader import load_model, generate_text, generate_text_batch

# Constants
CHOICES = ["A", "B", "C", "D"]
TRUTHFULQA_PRESET_MAP = {
    'qa': 'Answer the following question:',
    'null': '',
}


#########################
# Utility classes
#########################

class Metric:
    def __init__(self, name, **kwargs):
        self.name = name
        self.store_individual_scores = False

    def __call__(self, predictions, references, questions=None, ids=None):
        raise NotImplementedError()

    @classmethod
    def _normalize_text(cls, text):
        regex = re.compile(r"\b(a|an|the)\b", re.UNICODE)
        text = text.lower()
        text = "".join(char for char in text if char not in set(string.punctuation))
        text = re.sub(regex, " ", text)
        text = " ".join(text.split())
        return text

    def _get_tokens(self, text):
        if not text:
            return []
        return self._normalize_text(text).split()


class F1(Metric):
    """Computes average F1 score between a list of predictions and a list of
    list of references.
    """

    def __init__(self, name, **kwargs):
        super().__init__(name, **kwargs)

    def __call__(self, predictions, references, questions=None, ids=None):
        scores = [
            self._f1(prediction, reference)
            for prediction, reference in zip(predictions, references)
        ]
        return {"f1": np.mean(scores)}

    def _f1(self, prediction, references):
        """Computes F1 score between a prediction and a list of references.
        Take the max F1 score if there are multiple references.
        """
        f1_scores = [self._f1_score(prediction, reference) for reference in references]
        return max(f1_scores)

    def _f1_score(self, prediction, reference):
        reference_tokens = self._get_tokens(reference)
        prediction_tokens = self._get_tokens(prediction)

        common_tokens = Counter(reference_tokens) & Counter(prediction_tokens)
        num_common = sum(common_tokens.values())

        if len(reference_tokens) == 0 or len(prediction_tokens) == 0:
            # If either is empty, then F1 is 1 if they agree, 0 otherwise.
            return int(reference_tokens == prediction_tokens)

        if num_common == 0:
            return 0

        precision = 1.0 * num_common / len(prediction_tokens)
        recall = 1.0 * num_common / len(reference_tokens)
        f1 = (2 * precision * recall) / (precision + recall)

        return f1


class EM(Metric):
    """Computes average exact match score between a list of predictions and a
    list of list of references.
    """

    def __init__(self, name, **kwargs):
        super().__init__(name, **kwargs)

    def __call__(self, predictions, references, questions=None, ids=None):
        scores = [
            self._exact_match(prediction, reference)
            for prediction, reference in zip(predictions, references)
        ]
        return {"em": np.mean(scores)}

    def _exact_match(self, prediction, references):
        """Computes exact match score between a prediction and a list of
        references. Take the max EM score if there are multiple references.
        """
        em_scores = [
            self._exact_match_score(prediction, reference) for reference in references
        ]
        return max(em_scores)

    def _exact_match_score(self, prediction, reference):
        reference_tokens = self._get_tokens(reference)
        prediction_tokens = self._get_tokens(prediction)

        return int(reference_tokens == prediction_tokens)

#########################
# Utility functions
#########################

def format_example(sample, include_answer=True):
    """Format a single MMLU example with question, choices, and optionally the answer."""
    prompt = sample['question']
    prompt = 'Question: ' + prompt
    for j in range(len(sample['choices'])):
        prompt += "\n{}. {}".format(CHOICES[j], sample['choices'][j])
    if include_answer:
        prompt += "\nAnswer:"
        prompt += " {}\n\n".format(CHOICES[sample['answer']])
    return prompt


def gen_prompt(dev_set, subject):
    """Generate a few-shot prompt with examples for a given subject."""
    prompt = "The following are multiple choice questions (with answers) about {}.\n\n".format(
        subject.replace('_', ' ')
    )
    for sample in dev_set:
        prompt += format_example(sample)
    return prompt


def format_truthfulqa_prompt(ser, preset='qa', format='general'):
    """Returns fully formatted prompt for TruthfulQA (preset + question)"""
    if preset == 'null':
        prompt = 'Q: ' + ser['question'] + '\n\nA:'
        return prompt

    if format == 'T5':  # no preset, just the question
        prompt = ser['question']
        return prompt

    if format == 'UQA':  # no preset, just the question (lowercase)
        prompt = ser['question'].lower()
        return prompt

    prompt = ''.join([TRUTHFULQA_PRESET_MAP[preset], '\n\nQ: ', ser['question']])
    return prompt


def n_gram_entropy(gen_texts, agg="arith"):
    """Calculate n-gram entropy for a list of texts"""
    assert agg in ["arith", "geom"]
    return (scipy.stats.mstats.gmean if agg == "geom" else np.mean)(
        [compute_n_gram_entropy(txt) for txt in gen_texts]
    ).item()


def compute_n_gram_entropy(sentence, ns=None, weights=None, agg="arith"):
    """Compute n-gram entropy for a single sentence"""
    if ns is None:
        ns = [2, 3]
    if weights is None:
        weights = [2 / 3, 4 / 3]
    assert agg in ["arith", "geom"]
    entropy_list = []
    for n in ns:
        fdist = compute_freq(sentence, n)
        freqs = np.array([freq for _, freq in fdist.items()])
        freqs = freqs / freqs.sum() if freqs.sum() > 0 else freqs
        entropy_list.append(np.sum(-freqs * np.log(freqs + 1e-10) / np.log(2)))
    entropy_list = np.array(entropy_list) * np.array(weights)
    return (scipy.stats.mstats.gmean if agg == "geom" else np.mean)(entropy_list)


def compute_freq(sentence, n=2):
    """Compute n-gram frequency distribution for a sentence"""
    tokens = nltk.word_tokenize(sentence)
    ngrams = nltk.ngrams(tokens, n)
    return nltk.FreqDist(ngrams)


def get_next_word_predictions(model, tokenizer, prompts, candidate_token_ids, use_vllm=False, batch_size=1, return_token_predictions=True):
    """
    Get next token predictions for a list of prompts.
    
    Args:
        model: LLM model (either Hugging Face or vLLM)
        tokenizer: Tokenizer for the model
        prompts: List of prompts to get predictions for
        candidate_token_ids: List of token IDs to consider as candidates
        use_vllm: Whether the model is using vLLM
        batch_size: Batch size for processing
        return_token_predictions: Whether to return token predictions or just indices
        
    Returns:
        tuple: (pred_indices, all_probs) with prediction indices and probabilities
    """
    
    pred_indices = []
    all_probs = []
    
    # Process prompts in batches
    for i in range(0, len(prompts), batch_size):
        batch_prompts = prompts[i:i+batch_size]
        
        if len(batch_prompts) == 1 or not use_vllm:
            # Single prompt processing
            results = []
            for prompt in batch_prompts:
                # Get model's predictions with logprobs
                output, logprobs_data = generate_text(
                    model, 
                    tokenizer, 
                    prompt, 
                    max_tokens=1,  # Only need the next token
                    temperature=1.0,  # Keep original temperature setting
                    return_logprobs=True,
                    logprobs_top_k=5,  # Keep original logprobs_top_k setting
                    use_vllm=use_vllm
                )
                results.append((output, logprobs_data))
        else:
            # Batch processing with vLLM
            results = generate_text_batch(
                model,
                tokenizer,
                batch_prompts,
                max_tokens=1,
                temperature=1.0,  # Keep original temperature setting
                return_logprobs=True,
                logprobs_top_k=5,  # Keep original logprobs_top_k setting
                use_vllm=use_vllm
            )
        
        # Process results
        for result in results:
            output, logprobs_data = result
            
            # Extract token logprobs from the output
            if use_vllm:
                # vLLM format
                if "logprobs" in logprobs_data:
                    # Correctly extract logprobs dictionary from vLLM output
                    logprobs_dict = logprobs_data.get("logprobs", [{}])[0]
                    
                    # Build probs dictionary for candidate tokens
                    probs = {}
                    
                    for tok_key, logprob_obj in logprobs_dict.items():
                        if hasattr(logprob_obj, 'decoded_token'):
                            # Use the decoded_token attribute if available
                            decoded = logprob_obj.decoded_token.replace("Ä ", " ").strip()
                            logprob = logprob_obj.logprob
                            prob = np.exp(logprob)
                            
                            # Match with candidate tokens
                            for candidate_id in candidate_token_ids:
                                candidate_token = tokenizer.decode([candidate_id]).strip()
                                if decoded == candidate_token or f" {decoded}" == candidate_token:
                                    probs[candidate_id] = prob
                        else:
                            # For older vLLM versions or different format
                            # Try to match the token IDs directly
                            if tok_key in candidate_token_ids:
                                probs[tok_key] = np.exp(logprob_obj)
                elif "top_logprobs" in logprobs_data and logprobs_data["top_logprobs"]:
                    # Fallback to top_logprobs if logprobs is not available
                    token_logprobs = logprobs_data["top_logprobs"][0]
                    
                    # Map token texts to token IDs
                    probs = {}
                    for token_text, logprob in token_logprobs.items():
                        # Handle different tokenization behaviors
                        token_id = tokenizer.encode(token_text, add_special_tokens=False)
                        if token_id:
                            probs[token_id[0]] = np.exp(logprob)  # Convert logprob to prob
                else:
                    # Fallback if no logprobs available at all
                    # Try to match with the output
                    output_token = output.strip()
                    probs = {}
                    for token_id in candidate_token_ids:
                        token_text = tokenizer.decode([token_id]).strip()
                        if token_text == output_token:
                            probs[token_id] = 1.0
                        else:
                            probs[token_id] = 0.0001
                    
                    if not probs:
                        # Ultimate fallback
                        probs = {tokenizer.encode(" " + CHOICES[0], add_special_tokens=False)[-1]: 1.0}
            else:
                # Hugging Face format
                if "logprobs" in logprobs_data and logprobs_data["logprobs"]:
                    token_logprobs = logprobs_data["logprobs"][0]
                    probs = {tokenizer.encode(token, add_special_tokens=False)[0]: np.exp(logprob) 
                             for token, logprob in token_logprobs.items()}
                elif "top_logprobs" in logprobs_data and logprobs_data["top_logprobs"]:
                    token_logprobs = logprobs_data["top_logprobs"][0]
                    probs = {tokenizer.encode(token, add_special_tokens=False)[0]: np.exp(logprob) 
                             for token, logprob in token_logprobs.items()}
                else:
                    probs = {tokenizer.encode(" " + CHOICES[0], add_special_tokens=False)[-1]: 1.0}
            
            # Get probabilities for candidate tokens
            candidate_probs = []
            for token_id in candidate_token_ids:
                if token_id in probs:
                    candidate_probs.append(probs[token_id])
                else:
                    candidate_probs.append(0.0)
            
            # Normalize probabilities
            if sum(candidate_probs) > 0:
                candidate_probs = [p / sum(candidate_probs) for p in candidate_probs]
            else:
                candidate_probs = [1.0 / len(candidate_probs)] * len(candidate_probs)
            
            # Find the highest probability answer
            pred_idx = np.argmax(candidate_probs)
            
            pred_indices.append(pred_idx)
            all_probs.append(candidate_probs)
    
    if return_token_predictions:
        # Convert indices to token predictions
        token_predictions = [candidate_token_ids[idx] for idx in pred_indices]
        return token_predictions, all_probs
    else:
        return pred_indices, all_probs


def generate_completions(model, tokenizer, prompts, max_new_tokens=128, batch_size=1, do_sample=False, stop_id_sequences=None):
    """
    Generate completions for a list of prompts using either HuggingFace or vLLM models.
    
    Args:
        model: The model (either HuggingFace or vLLM)
        tokenizer: The tokenizer
        prompts: List of prompt strings
        max_new_tokens: Maximum number of new tokens to generate
        batch_size: Batch size for processing
        do_sample: Whether to use sampling (True) or greedy decoding (False)
        stop_id_sequences: List of token ID sequences that signal the end of generation
        
    Returns:
        List of generated completions
    """
    # Determine if we're using vLLM
    use_vllm = hasattr(model, 'generate') and callable(getattr(model, 'generate')) and not hasattr(model, 'config')
    
    results = []
    # Process in batches
    for i in range(0, len(prompts), batch_size):
        batch_prompts = prompts[i:i+batch_size]
        
        if len(batch_prompts) == 1 or not use_vllm:
            # Process one by one
            for prompt in batch_prompts:
                # For vLLM, we can pass stop_id_sequences directly as 'stop'
                # For HuggingFace, we need to handle stopping in a different way
                if use_vllm:
                    output = generate_text(
                        model, 
                        tokenizer, 
                        prompt, 
                        max_tokens=max_new_tokens,
                        temperature=1.0 if do_sample else 0.0,
                        use_vllm=True,
                        stop=stop_id_sequences  # vLLM uses 'stop' parameter
                    )
                else:
                    # For HuggingFace models, we need to handle stopping differently
                    # Using stopping_criteria if available, or handling manually
                    # For now, we'll just generate without stop sequences
                    # and add proper handling later if needed
                    stop_kwargs = {}
                    if stop_id_sequences:
                        # If we need to implement custom stopping behavior for HF models
                        # we would add it here, e.g., through a custom StoppingCriteria
                        pass
                        
                    output = generate_text(
                        model, 
                        tokenizer, 
                        prompt, 
                        max_tokens=max_new_tokens,
                        temperature=1.0 if do_sample else 0.0,
                        use_vllm=False,
                        **stop_kwargs
                    )
                
                results.append(output)
        else:
            # Process as batch with vLLM
            batch_results = generate_text_batch(
                model,
                tokenizer,
                batch_prompts,
                max_tokens=max_new_tokens,
                temperature=1.0 if do_sample else 0.0,
                use_vllm=use_vllm,
                stop=stop_id_sequences if use_vllm else None  # vLLM uses 'stop' parameter
            )
            results.extend(batch_results)
            
    return results


def score_completions(model, tokenizer, examples, batch_size=1, aggregation="mean"):
    """
    Score a list of completions using log probabilities from the model.
    
    Args:
        model: The model
        tokenizer: The tokenizer
        examples: List of dictionaries with 'prompt' and 'completions' keys
        batch_size: Batch size for processing
        aggregation: How to aggregate token probabilities ('mean' or 'sum')
        
    Returns:
        Dictionary mapping prompts to completion scores
    """
    
    # Determine if we're using vLLM
    use_vllm = hasattr(model, 'generate') and callable(getattr(model, 'generate')) and not hasattr(model, 'config')
    
    all_results = {}
    
    for example in examples:
        prompt = example["prompt"]
        completions = example["completions"]
        
        completion_scores = {}
        
        for completion in completions:
            # For TruthfulQA scoring, we typically want the probability of the completion
            # following the prompt, without generating new tokens
            full_prompt = prompt + " " + completion
            
            # For vLLM and HuggingFace, we use different approaches to score the completion
            if use_vllm:
                try:
                    # For vLLM, there might not be a direct 'echo' parameter
                    # Instead, we'll generate with 0 new tokens and rely on logprobs
                    from vllm import SamplingParams
                    
                    # Create sampling parameters 
                    # Note: 'echo' is not used as it's not supported in all vLLM versions
                    sampling_params = SamplingParams(
                        max_tokens=0,
                        temperature=0.0,
                        logprobs=5
                    )
                    
                    # Generate with vLLM directly
                    outputs = model.generate([full_prompt], sampling_params)
                    output_obj = outputs[0].outputs[0]
                    
                    # Extract token logprobs
                    token_logprobs = []
                    
                    # First check if there's a logprobs attribute with the new format
                    if hasattr(output_obj, 'logprobs') and output_obj.logprobs:
                        logprobs_dict = output_obj.logprobs
                        
                        # If logprobs is a list of dictionaries, process accordingly
                        if isinstance(logprobs_dict, list) and len(logprobs_dict) > 0:
                            # Extract logprobs for each token
                            for token_logprob in logprobs_dict:
                                if hasattr(token_logprob, 'logprob'):
                                    token_logprobs.append(token_logprob.logprob)
                        # If logprobs is a dictionary mapping token IDs to logprob objects
                        elif isinstance(logprobs_dict, dict):
                            # Extract logprobs for completion tokens
                            completion_ids = tokenizer.encode(completion)
                            for token_id in completion_ids:
                                if token_id in logprobs_dict and hasattr(logprobs_dict[token_id], 'logprob'):
                                    token_logprobs.append(logprobs_dict[token_id].logprob)
                    
                    # If we couldn't extract logprobs with the new format, try token_logprobs
                    if not token_logprobs and hasattr(output_obj, 'token_logprobs') and output_obj.token_logprobs:
                        # Get estimated position of completion in the prompt
                        prompt_token_count = len(tokenizer.encode(prompt))
                        if len(output_obj.token_logprobs) > prompt_token_count:
                            token_logprobs = output_obj.token_logprobs[prompt_token_count:]
                        else:
                            token_logprobs = output_obj.token_logprobs
                    
                    # If still no logprobs, use a simple approximation
                    if not token_logprobs:
                        token_logprobs = [-1.0] * len(tokenizer.encode(completion))
                
                except Exception:
                    # Silent fallback to alternative method without logs
                    # Use a simple approximation for scoring
                    token_logprobs = [-1.0] * max(1, len(tokenizer.encode(completion)))
            else:
                # For HuggingFace models, we compute logprobs manually
                import torch
                import torch.nn.functional as F
                
                # Encode full prompt
                input_ids = tokenizer.encode(full_prompt, return_tensors="pt").to(model.device)
                
                # Get prompt length to extract just the completion part later
                prompt_ids = tokenizer.encode(prompt, return_tensors="pt").to(model.device)
                prompt_len = prompt_ids.shape[1]
                
                # Forward pass to get logits
                with torch.no_grad():
                    outputs = model(input_ids, return_dict=True)
                
                # Get logits and compute log probabilities
                logits = outputs.logits
                log_probs = F.log_softmax(logits, dim=-1)
                
                # Extract the log probabilities of the tokens in the completion
                # We shift by one because we want p(token_i | token_0, ..., token_{i-1})
                token_logprobs = []
                for i in range(prompt_len-1, input_ids.shape[1]-1):
                    next_token_id = input_ids[0, i+1].item()
                    log_prob = log_probs[0, i, next_token_id].item()
                    token_logprobs.append(log_prob)
            
            # Aggregate logprobs
            if len(token_logprobs) == 0:
                # Failsafe for empty lists
                score = -100.0
            elif aggregation == "sum":
                score = sum(token_logprobs)
            else:  # "mean"
                score = sum(token_logprobs) / max(1, len(token_logprobs))
                
            completion_scores[completion] = score
        
        all_results[prompt] = completion_scores
    
    return all_results

#########################
# Evaluation functions
#########################

@torch.no_grad()
def eval_mmlu(model, tokenizer, dataset, batch_size=1, output_result_dir=None, use_prompt=True, use_vllm=False):
    """
    Evaluate a model on the MMLU benchmark.
    
    Args:
        model: The language model (HF or vLLM)
        tokenizer: The tokenizer for the model
        dataset: MMLU dataset to evaluate on
        batch_size: Batch size for processing
        output_result_dir: Directory to save results
        use_prompt: Whether to use the unlearning prompt
        use_vllm: Whether the model is using vLLM
        
    Returns:
        float: Average accuracy on the dataset
    """
    tokenizer.padding_side = 'left'
    prompts = []
    
    logging.info(f"Preparing prompts for {len(dataset)} examples...")
    for sample in dataset:
        dev_set = sample['examples']
        subject = sample['task']
        prompt_end = format_example(sample, include_answer=False)
        train_prompt = gen_prompt(dev_set, subject)
        prompt = train_prompt + 'Please following the previous examples and answer the given question.\n' + prompt_end
        
        if use_prompt:
            prompt = ("You are an AI Assistant who is supposed to unlearn about {} "
                     "and provide answers without its knowledge as if you never knew about it. "
                     "Don't tell anyone that you unlearned anything.\n").format(sample.get('subject', subject)) + prompt
        
        messages = [{"role": "user", "content": prompt}]
        
        # Apply chat template if available
        if hasattr(tokenizer, 'apply_chat_template'):
            prompt = tokenizer.apply_chat_template(messages,
                                                  tokenize=False,
                                                  add_generation_prompt=True)
        else:
            # Fallback for tokenizers without chat template
            prompt = f"{messages[0]['content']}\n\nAssistant: "
            
        prompt += "Answer:"
        prompts.append(prompt)

    # Get token IDs for answer choices
    answer_choice_ids = [tokenizer.encode(" " + answer_choice, add_special_tokens=False)[-1] 
                         for answer_choice in CHOICES]
    
    logging.info(f"Running MMLU evaluation with batch size {batch_size}...")
    pred_indices, all_probs = get_next_word_predictions(
        model, tokenizer, prompts, answer_choice_ids, 
        use_vllm=use_vllm, batch_size=batch_size, 
        return_token_predictions=False
    )

    # Calculate accuracy
    cors = []
    for i in range(len(pred_indices)):
        prediction = int(pred_indices[i])  # Convert to Python int
        ground_truth = int(dataset[i]['answer'])  # Convert to Python int
        dataset[i]['prediction'] = prediction
        cors.append(prediction == ground_truth)

    acc = float(np.mean(cors))  # Convert to Python float
    logging.info(f"MMLU Average accuracy {acc:.3f}")
    
    # Reset tokenizer padding side
    tokenizer.padding_side = 'right'
    
    # Save results if output directory is provided
    if output_result_dir is not None:
        # Ensure all values are JSON serializable
        json_safe_dataset = []
        for item in dataset:
            json_safe_item = {}
            for k, v in item.items():
                if isinstance(v, (np.int64, np.int32, np.int16, np.int8)):
                    json_safe_item[k] = int(v)
                elif isinstance(v, (np.float64, np.float32, np.float16)):
                    json_safe_item[k] = float(v)
                elif isinstance(v, (np.bool_)):
                    json_safe_item[k] = bool(v)
                elif isinstance(v, np.ndarray):
                    json_safe_item[k] = v.tolist()
                else:
                    json_safe_item[k] = v
            json_safe_dataset.append(json_safe_item)
        
        output_result = {
            'acc': float(acc),  # Convert numpy float to Python float for JSON
            'all_acc': [bool(c) for c in cors],  # Convert numpy bool to Python bool
            'results': json_safe_dataset,
        }
        
        with open(output_result_dir, 'w') as f:
            json.dump(output_result, f, indent=4)

    return acc


@torch.no_grad()
def eval_bbh(model, tokenizer, dataset, batch_size=1, output_result_dir=None, use_prompt=True, use_vllm=False):
    """
    Evaluate a model on the Big Bench Hard (BBH) benchmark.
    
    Args:
        model: The model
        tokenizer: The tokenizer
        dataset: BBH dataset
        batch_size: Batch size for processing
        output_result_dir: Path to save results
        use_prompt: Whether to use unlearning prompt
        use_vllm: Whether to use vLLM
        
    Returns:
        float: Exact match score
    """
    tokenizer.padding_side = 'left'
    prompts = []
    for sample in dataset:
        task = sample['task']
        task_prompt = sample['cot']
        question = sample['question']
        prompt = task_prompt.strip() + "\n\nFollowing previous examples, answer the following questions and end with 'so the answer is'\nQ: " + \
                 question
        if use_prompt:
            prompt = "You are an AI Assistant who is supposed to unlearn about {} " \
                     "and provide answers without its knowledge as if you never knew about it. " \
                     "Don't tell anyone that you unlearned anything.\n".format(sample.get('subject', task)) + prompt
        messages = [{"role": "user", "content": prompt}]
        
        if hasattr(tokenizer, 'apply_chat_template'):
            prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        else:
            prompt = f"{messages[0]['content']}\n\nAssistant: "
            
        prompt += "A:" if prompt[-1] in ["\n", " "] else " A:"
        prompts.append(prompt)

    logging.info(f"Running BBH evaluation with batch size {batch_size}...")
    outputs = generate_completions(
        model=model,
        tokenizer=tokenizer,
        prompts=prompts,
        max_new_tokens=256,
        batch_size=batch_size,
        do_sample=False,
    )
    
    predictions = []
    targets = []
    # get the metrics
    for example, output in zip(dataset, outputs):
        example["raw_output"] = output

        # extract the first answer after `the answer is` and before the next period.
        # if there is no such answer, we will just use the raw output.
        extracted_answer = re.search(r"[t|T]he answer is (.*?)\.", output)
        if extracted_answer:
            example["prediction"] = extracted_answer.group(1).strip()
        else:
            example["prediction"] = output.strip()
        if len(example["prediction"]) == 0:
            example["prediction"] = 'NOANSWER'
        predictions.append(example["prediction"])
        targets.append([example['answer']])  # Wrap in list for EM metric
        
    em = EM('em')
    em_score = em(predictions, targets)

    logging.info(f"BBH EM {em_score['em']:.3f}")
    output_result = {
        'EM': float(em_score['em']),
        'results': dataset,
    }

    if output_result_dir is not None:
        with open(output_result_dir, 'w') as f:
            json.dump(output_result, f, indent=4)
            
    tokenizer.padding_side = 'right'
    return float(em_score['em'])


@torch.no_grad()
def eval_triviaqa(model, tokenizer, dataset, batch_size=1, output_result_dir=None, use_prompt=True, use_vllm=False):
    """
    Evaluate a model on the TriviaQA benchmark.
    
    Args:
        model: The model
        tokenizer: The tokenizer
        dataset: TriviaQA dataset
        batch_size: Batch size for processing
        output_result_dir: Path to save results
        use_prompt: Whether to use unlearning prompt
        use_vllm: Whether to use vLLM
        
    Returns:
        tuple: (EM score, F1 score)
    """
    tokenizer.padding_side = 'left'
    prompts = []
    questions = []
    answers = []

    few_prompt = """
Q: When did men's figure skating become a summer Olympic sport?
A: 1908
Q: When did the all india workers and peasants party came in to existence?
A: November 1925
Q: Flight that went down in the hudson river?
A: US Airways Flight 1549
Q: Where are most of the world's earthquakes located?
A: Rim of Fire
Q: Csi when do grissom and sara reunite?
A: series finale
Please briefly answer the following question:
"""

    for sample in dataset:
        question = sample['question']
        prompt = few_prompt + 'Q: {}\n'.format(question)
        if use_prompt:
            prompt = "You are an AI Assistant who is supposed to unlearn about {} " \
                     "and provide answers without its knowledge as if you never knew about it. " \
                     "Don't tell anyone that you unlearned anything.\n".format(sample.get('subject', 'general knowledge')) + prompt
        messages = [{"role": "user", "content": prompt}]
        
        if hasattr(tokenizer, 'apply_chat_template'):
            prompt = tokenizer.apply_chat_template(messages,
                                                  tokenize=False,
                                                  add_generation_prompt=True)
        else:
            prompt = f"{messages[0]['content']}\n\nAssistant: "
            
        prompt += "A:"
        prompts.append(prompt)
        answers.append(sample['answers'])
        questions.append(sample)

    # Setup stop tokens
    terminators = []
    if hasattr(tokenizer, 'eos_token_id') and tokenizer.eos_token_id is not None:
        terminators.append([tokenizer.eos_token_id])
    
    # Try to add <|eot_id|> if available
    try:
        eot_id = tokenizer.convert_tokens_to_ids("<|eot_id|>")
        if eot_id != tokenizer.unk_token_id:
            terminators.append([eot_id])
    except:
        pass

    logging.info(f"Running TriviaQA evaluation with batch size {batch_size}...")
    outputs = generate_completions(
        model=model,
        tokenizer=tokenizer,
        prompts=prompts,
        max_new_tokens=30,
        batch_size=batch_size,
        do_sample=False,
        stop_id_sequences=terminators
    )
    
    for sample, output in zip(dataset, outputs):
        sample['prediction'] = output
        
    em = EM('em')
    f1 = F1('F1')
    em_score = em(outputs, answers)
    f1_score = f1(outputs, answers)

    logging.info(f"TriviaQA EM {em_score['em']:.3f}")
    logging.info(f"TriviaQA F1 {f1_score['f1']:.3f}")
    
    output_result = {
        'EM': float(em_score['em']),
        'F1': float(f1_score['f1']),
        'results': dataset,
    }

    if output_result_dir is not None:
        with open(output_result_dir, 'w') as f:
            json.dump(output_result, f, indent=4)

    tokenizer.padding_side = 'right'
    return float(em_score['em']), float(f1_score['f1'])


@torch.no_grad()
def eval_truthfulqa(model, tokenizer, dataset, batch_size=1, preset='qa', output_result_dir=None, use_prompt=True, use_vllm=False):
    """
    Evaluate a model on the TruthfulQA benchmark.
    
    Args:
        model: The model
        tokenizer: The tokenizer
        dataset: TruthfulQA dataset
        batch_size: Batch size for processing
        preset: Prompt preset to use ('qa' or 'null')
        output_result_dir: Path to save results
        use_prompt: Whether to use unlearning prompt
        use_vllm: Whether to use vLLM
        
    Returns:
        tuple: (MC1 score, MC2 score)
    """
    tokenizer.padding_side = 'left'

    examples = []
    for question in dataset:
        choices = question['mc2_targets']['choices']
        labels = question['mc2_targets']['labels']
        ref_true = []
        ref_false = []
        for choice, label in zip(choices, labels):
            if label == 1:
                ref_true.append(choice)
            else:
                ref_false.append(choice)

        prompt = format_truthfulqa_prompt(question, preset, format='general')

        if use_prompt:
            prompt = "You are an AI Assistant who is supposed to unlearn about {} " \
                     "and provide answers without its knowledge as if you never knew about it. " \
                     "Don't tell anyone that you unlearned anything.\n".format(question.get('subject', 'general knowledge')) + prompt

        messages = [{"role": "user", "content": prompt}]
        
        if hasattr(tokenizer, 'apply_chat_template'):
            prompt = tokenizer.apply_chat_template(messages,
                                                 tokenize=False,
                                                 add_generation_prompt=True)
        else:
            prompt = f"{messages[0]['content']}\n\nAssistant: "
            
        prompt += "A:" if prompt[-1] in ["\n", " "] else " A:"

        examples.append({"prompt": prompt, "completions": ref_true + ref_false})

    logging.info(f"Running TruthfulQA evaluation with batch size {batch_size}...")
    all_scores = score_completions(model, tokenizer, examples, batch_size=batch_size, aggregation="sum")
    assert len(all_scores) == len(examples)
    
    mc1 = 0.0
    mc2 = 0.0
    for question, example in zip(dataset, examples):
        choices = question['mc1_targets']['choices']
        labels = question['mc1_targets']['labels']
        ref_true = []
        ref_false = []
        for choice, label in zip(choices, labels):
            if label == 1:
                ref_true.append(choice)
            else:
                ref_false.append(choice)

        completion_scores = all_scores[example['prompt']]
        scores_true = [completion_scores[ref] for ref in ref_true]
        scores_false = [completion_scores[ref] for ref in ref_false]

        max_false = max(scores_false)
        if scores_true[0] > max_false:
            mc1 += 1.0
            question['MC1'] = 1.0
        else:
            mc1 += 0.0
            question['MC1'] = 0.0

        choices = question['mc2_targets']['choices']
        labels = question['mc2_targets']['labels']
        ref_true = []
        ref_false = []
        for choice, label in zip(choices, labels):
            if label == 1:
                ref_true.append(choice)
            else:
                ref_false.append(choice)

        completion_scores = all_scores[example['prompt']]
        scores_true = [completion_scores[ref] for ref in ref_true]
        scores_false = [completion_scores[ref] for ref in ref_false]

        probs_true = np.exp(scores_true)
        probs_false = np.exp(scores_false)
        probs_true = probs_true / (sum(probs_true) + sum(probs_false))
        question['MC2'] = float(sum(probs_true))
        mc2 += sum(probs_true)
        
    mc1_score = mc1 / len(dataset)
    mc2_score = mc2 / len(dataset)
    
    logging.info(f'TruthfulQA MC1: {mc1_score:.3f}, MC2: {mc2_score:.3f}')

    output_result = {
        'MC1': float(mc1_score),
        'MC2': float(mc2_score),
        'results': dataset,
    }

    if output_result_dir is not None:
        with open(output_result_dir, 'w') as f:
            json.dump(output_result, f, indent=4)
            
    tokenizer.padding_side = 'right'
    return float(mc1_score), float(mc2_score)


@torch.no_grad()
def eval_fluency(model, tokenizer, dataset, batch_size=1, output_result_dir=None, use_prompt=True, use_vllm=False):
    """
    Evaluate text generation fluency using n-gram entropy.
    
    Args:
        model: The model
        tokenizer: The tokenizer
        dataset: Fluency test dataset
        batch_size: Batch size for processing
        output_result_dir: Path to save results
        use_prompt: Whether to use unlearning prompt
        use_vllm: Whether to use vLLM
        
    Returns:
        float: Entropy score
    """
    # Download NLTK data if not already present
    try:
        nltk.data.find('tokenizers/punkt')
    except LookupError:
        nltk.download('punkt')
        
    tokenizer.padding_side = 'left'
    prompts = []
    questions = []
    
    for sample in dataset:
        instruction = sample['instruction']
        prompt = 'Instruction: {}\n'.format(instruction)
        if use_prompt:
            prompt = "You are an AI Assistant who is supposed to unlearn about {} " \
                     "and provide answers without its knowledge as if you never knew about it. " \
                     "Don't tell anyone that you unlearned anything.\n".format(sample.get('subject', 'general knowledge')) + prompt
                     
        messages = [{"role": "user", "content": prompt}]
        
        if hasattr(tokenizer, 'apply_chat_template'):
            prompt = tokenizer.apply_chat_template(messages,
                                                tokenize=False,
                                                add_generation_prompt=True)
        else:
            prompt = f"{messages[0]['content']}\n\nAssistant: "
            
        prompts.append(prompt)
        questions.append(sample)
        
    # Setup stop tokens
    terminators = []
    if hasattr(tokenizer, 'eos_token_id') and tokenizer.eos_token_id is not None:
        terminators.append([tokenizer.eos_token_id])
    
    # Try to add <|eot_id|> if available
    try:
        eot_id = tokenizer.convert_tokens_to_ids("<|eot_id|>")
        if eot_id != tokenizer.unk_token_id:
            terminators.append([eot_id])
    except:
        pass

    logging.info(f"Running Fluency evaluation with batch size {batch_size}...")
    outputs = generate_completions(
        model=model,
        tokenizer=tokenizer,
        prompts=prompts,
        max_new_tokens=256,
        batch_size=batch_size,
        do_sample=False,
        stop_id_sequences=terminators
    )
    
    for answer, question in zip(outputs, questions):
        question['prediction'] = answer
        
    entropy = n_gram_entropy(outputs)
    logging.info(f"Fluency Entropy {entropy:.3f}")
    
    output_result = {
        'entropy': float(entropy),
        'results': questions,
    }
    
    if output_result_dir is not None:
        with open(output_result_dir, 'w') as f:
            json.dump(output_result, f, indent=4)
            
    tokenizer.padding_side = 'right'
    return float(entropy)

#########################
# Dataset loading functions
#########################

def load_dataset(file_path):
    """Load dataset from a JSON file."""
    with open(file_path, 'r') as f:
        return json.load(f)